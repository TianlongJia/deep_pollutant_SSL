{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix for CocoAPI\n",
    "\n",
    "The \"main_Evaluate_Object_Detection.ipynb\" uses the CocoApi to evaluate the model performance, e.g., AP50. But that code does not include the output of confusion matrix (e.g., TP, FP, and FN).\n",
    "\n",
    "This script provides the output of confusion matrix using **FiftyOne** tool\n",
    "\n",
    "FiftyOne’s implementation of COCO-style evaluation matches the reference implementation available via pycocotools (https://github.com/cocodataset/cocoapi).\n",
    "\n",
    "Refer to: \n",
    "https://docs.voxel51.com/integrations/coco.html#loading-coco-formatted-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install fiftyone and cocoapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   For windows:\n",
    "#        pip3 install fiftyone --user\n",
    "\n",
    "#   For windows:\n",
    "#        install pycocotools-2.0:\n",
    "#        pip install git+https://github.com/philferriere/cocoapi.git#egg=pycocotools^&subdirectory=PythonAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load coco dataset (ground-truth)\n",
    "\n",
    "**Note**: \n",
    "\n",
    "(1) the folder/path of images in json file must contains \" / \" instead of \" \\ \" \n",
    "\n",
    "(2) In json file, \"image_id\" and the \"id\" for annotations must not start \"0\", otherwise the results will be not correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████████| 29/29 [1.8s elapsed, 0s remaining, 16.2 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "# A name for the dataset\n",
    "# name = \"my-dataset\"\n",
    "\n",
    "# GJO test dataset\n",
    "image_path = r\"U:/AIMMW/Tianlong/Peng/Paper_SSL/Dataset/labeled_data/subsets/Test/test\"\n",
    "labels_path = r\"U:/AIMMW/Tianlong/Peng/Paper_SSL/Dataset/labeled_data/subsets/Test/test.json\"\n",
    "\n",
    "# The type of the dataset being imported\n",
    "dataset_type = fo.types.COCODetectionDataset  # coco format\n",
    "\n",
    "dataset = fo.Dataset.from_dir(\n",
    "    dataset_type=dataset_type,\n",
    "    data_path=image_path,\n",
    "    labels_path=labels_path,\n",
    "    # include_id=True,\n",
    "    # name=name,\n",
    ")\n",
    "# print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the first data in the dataset,\n",
    "# Carefully check the \"label\" name!\n",
    "# Note the coordinates of the bbox is scaled into [0,1]\n",
    "\n",
    "# print(dataset.first().detections.detections[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load predictions\n",
    "\n",
    "When inferencing a model on a test dataset using Detectron2, it will output a prediction file named \"coco_instances_results.json\" with predicted bbox\n",
    "\n",
    "Note: when load predictions in a json file, the \"id\" in \"images\" field must start from **\"1\"**, Otherwise it will not work. You may change the \"id in json using: *https://github.com/TianlongJia/Tools/tree/master/Vision/Manage_pic_and_folder//Modify_JSON.ipynb*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes:  ['0', 'entrap bean', 'free bean']\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "import fiftyone.utils.coco as fouc\n",
    "import json\n",
    "\n",
    "#\n",
    "# Mock COCO predictions, where:\n",
    "# - `image_id` corresponds to the `coco_id` field of `coco_dataset`\n",
    "# - `category_id` corresponds to classes in `coco_dataset.default_classes`\n",
    "#\n",
    "# predictions = [\n",
    "#     {\"image_id\": 1, \"category_id\": 18, \"bbox\": [258, 41, 348, 243], \"score\": 0.87},\n",
    "#     {\"image_id\": 2, \"category_id\": 11, \"bbox\": [61, 22, 504, 609], \"score\": 0.95},\n",
    "# ]\n",
    "\n",
    "json_file= r\"C:\\Users\\tjian\\Desktop\\test_results\\coco_instances_results.json\"\n",
    "\n",
    "with open(json_file, 'r') as fcc_file:\n",
    "    fcc_data = json.load(fcc_file)\n",
    "# print(fcc_data)\n",
    "    \n",
    "predictions = fcc_data\n",
    "\n",
    "\n",
    "# Add COCO predictions to `predictions` field of dataset\n",
    "classes = dataset.default_classes\n",
    "# classes = ['hold_space', 'entrap bean', 'free bean',]\n",
    "print(\"classes: \", classes)\n",
    "\n",
    "fouc.add_coco_labels(dataset, \n",
    "                    #  coco_id_field=\"detections\", \n",
    "                     label_field=\"predictions\", \n",
    "                     labels_or_path=predictions, \n",
    "                     classes=classes)\n",
    "\n",
    "# # Verify that predictions were added to two images\n",
    "print(dataset.count(\"predictions\"))  # 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.values([dataset.detections, \"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset.count(\"detections\"))  # the number of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the first data (with prediction bbox) in the dataset ,\n",
    "# Carefully check the \"label\" name!\n",
    "# Note the coordinates of the predicted bbox is scaled into [0,1]\n",
    "\n",
    "# print(dataset.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = dataset.first()\n",
    "# print(sample.detections.detections[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output results \n",
    "\n",
    "by comparing ground-truth labels and predictions bbox\n",
    "\n",
    "**Note**: Predicted and ground truth objects are matched using a specified IoU threshold (default = 0.50). This threshold can be customized via the iou parameter. The IoU selection affects the calculation of TP, FN, are FP value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections...\n",
      " 100% |███████████████████| 29/29 [487.2ms elapsed, 0s remaining, 59.5 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████████| 29/29 [975.4ms elapsed, 0s remaining, 29.7 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "# Evaluate the objects in the `predictions` field with respect to the\n",
    "# objects in the `ground_truth` field\n",
    "results = dataset.evaluate_detections(\n",
    "    \"predictions\",\n",
    "    gt_field=\"detections\",\n",
    "    iou=0.5,\n",
    "    method=\"coco\",   \n",
    "    eval_key=\"eval\",\n",
    "    compute_mAP=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mAP\n",
    "\n",
    "Note: this is the mAP at IoU=.50:.05:.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30267943877876013\n"
     ]
    }
   ],
   "source": [
    "print(results.mAP())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TP, FP, FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 46\n",
      "FP: 35\n",
      "FN: 17\n"
     ]
    }
   ],
   "source": [
    "# Print some statistics about the total TP/FP/FN counts\n",
    "print(\"TP: %d\" % dataset.sum(\"eval_tp\"))\n",
    "print(\"FP: %d\" % dataset.sum(\"eval_fp\"))\n",
    "print(\"FN: %d\" % dataset.sum(\"eval_fn\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, recall, F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   free bean       0.62      0.92      0.74        39\n",
      " entrap bean       0.43      0.42      0.43        24\n",
      "\n",
      "   micro avg       0.57      0.73      0.64        63\n",
      "   macro avg       0.53      0.67      0.58        63\n",
      "weighted avg       0.55      0.73      0.62        63\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classes = 2\n",
    "counts = dataset.count_values(\"detections.detections.label\")\n",
    "classes = sorted(counts, key=counts.get, reverse=True)[:classes]\n",
    "\n",
    "# Print a classification report\n",
    "results.print_report(classes=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = results.plot_pr_curves(classes=classes)\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "\n",
    "# Perform evaluation, allowing objects to be matched between classes\n",
    "results = dataset.evaluate_detections(\n",
    "    \"predictions\",\n",
    "    gt_field=\"detections\",\n",
    "    method=\"coco\",\n",
    "    classwise=True,\n",
    ")\n",
    "\n",
    "# Generate a confusion matrix for the specified classes\n",
    "plot = results.plot_confusion_matrix(classes = classes)\n",
    "plot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
